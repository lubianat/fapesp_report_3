# Achievements {.page_break_before}

In the second year of the project, we addressed 2 different facets of the project. 

The first facet was the development of guidelines for describing new cell types in published research in partnership with the Cell Ontology. 

The second facet was the formalization of a biocuration routine to curate new cell types on Wikidata, an effort with 2 main products: the consolidation of Wikidata as a database for cell types and Wikidata Bib, a software and workflow for reading large amounts of scientific literature.

Additionally, we collaborated in several different projects related to organizing biomedical knowledge on Wikidata, which are also detailed in this session. 
## Biocuration of cell classes with Wikidata Bib  {.page_break_before}
### Introduction 

Reading scientific articles is an integral part of the routine of modern scientists.
Although several literature-management software are available [@url:https://en.wikipedia.org/wiki/Comparison_of_reference_management_software], the process of reading is mainly artisanal. 
There are no standard guidelines on how to probe the literature organize notes for biomedical researchers. 
Thus, while reading and studying is a core activity, there are few (if any) protocols for the efficient screening of scientific articles. 

Other professional traditions have dealt with similar issues in the past. 
Notetaking is vital to keep track of financial balances and avoid costly problems in accounting.
Double-entry bookkeeping was developed in the 13th century as a professional solution for notetaking in accounting where "every entry to an account requires a corresponding and opposite entry to a different account." [@url:https://en.wikipedia.org/w/index.php?title=Double-entry_bookkeeping&oldid=1055066428]
In software development, Test-Driven Development (TDD) is a popular methodology where tests for code snippets are written before the code itself, therefore ensuring that written software passes minimum quality standards.
The similarities of Double-entry bookkeeping and TDD are diverse [@url:https://blog.cleancoder.com/uncle-bob/2017/12/18/Excuses.html], but for our purpose, here suffices to see both as professionalized systems that promote better quality and accountability of works. 


In the humanities, there is a well-established practice of annotations of readings. 
The annotation skills are part of standard academic training in the humanities [@url:https://bibliotecadaeca.wordpress.com/2019/09/30/como-fazer-um-fichamento][@url:"https://www.youtube.com/playlist?list=PLAudUnJeNg4vWJhEJ_da26C-QW5qiS7uZ"]. 
An influential work in presenting methods for academic reading in the humanities is Umberto Eco's book "How to Write a Thesis" [@wikidata:Q3684178], which outlines not only _how_ to annotate the literature that basis an academic thesis, but also _why_ to do so. 
The book, written originally in 1977, is still influential today. 
Still, its theoretical scope (roughly the humanities) and its date preceding the digital era limits the extent to which it applies to the biomedical sciences. 


Notably, the need for an organized reading system for biocuration studies stems from a difference in methodology. 
In humanities, the main (if not sole) research material is the written text, the books and articles from which research stemsâ€”[@url:"https://www.youtube.com/playlist?list=PLAudUnJeNg4vWJhEJ_da26C-QW5qiS7uZ"].
In the biomedical sciences, including a large part of bioinformatics, the object of study is the natural world, observed via experimentation. 
Thus, naturally, scientific training focuses on experimentation and data analysis's theoretical and practical basis. 
With the boom of scientific articles, however, the scientific literature (and accompanying public datasets) already provide a strong material for sculpting scientific projects.
Thus, developing a methodology for academic reading tailored to the digital environment is a need. 

To address this gap, we are developing Wikidata Bib, a framework for large scale reading of scientific articles. 
It is presented in three parts, each with a technical overview alongside the theoretical foundations. 
First, Wikidata Bib is presented as a reading system for managing references and notes using a GitHub repository and plain text notes.
Then, we present how the system ensures accountability, allowing users to get personalized analytics on their reading patterns.
Finally, we demonstrate how Wikidata Bib fits an active curation environment, connecting the framework with the larger goal of this project of curating information about cell types on Wikidata.  

## Wikidata Bib as a reading system 

The reading framework of Wikidata bib is built upon a git repository integrated with GitHub, Python 3 scripts and SPARQL queries. The code is packaged into a python module to facilitate usage. It also uses the Click library to implement a professional Command Line Interface for end users (https://github.com/pallets/click).
It has a standard file structure, summarized as the following:

* `docs/`
    * `index.html`
* `downloads/`
    * `10.7554_ELIFE.52614.pdf`
* `src/`
    * `data/`
      * `config.yaml`
      * `index.yaml`
      * `read.csv`
      * `read.ttl`
      * `toread.yaml`
    * `notes/`
        * `Q87830400.md`
    * `wikidata_bib/`   
      * `get_pdf.py`
      * `read_paper.py`
      * `update_dashboard.py`
      * ...
* `LICENSE`
* `pyproject.toml`
* `README.md`
* `setup.cfg`

The `docs/` directory contains the live dashboard from the readings, which will be discussed in the following sessions.
The `downloads/` directory hosts the pdfs of the articles read with the system. 
These are not committed to the repository and are only stored locally. 
The `src/` directory contains a `data/` subfolder with the configuration files and the local database of what is read, a `notes` subfolder with the notes on the read articles and a `wikidata_bib` directory containing the actualpython code with the system's mechanics. 

After installing the package using the `pip` utility (<https://pypi.org/>), the user is able to use Wikidata Bib from the terminal as any other command line utility. 
The basic commands available are described below:
    - `bib read`  receives a Wikidata QID for an article and opens the article web page on a browser and opens a personalized notes document. It also updates the static files for the analytics dashboard, stored in the `docs/` directory.
    - `bib pop`, which "pops" an article from the reading stash at `toread.yaml` and runs `bib read` for it
    - `bib log` updates the git repository and adds, commits and pushes notes and the dashboard to  to GitHub

To customize the system, a new user would need to update 3 `yaml` files in the `src/data` folder: 
`toread.yaml`, `index.yaml`and `config.yaml`. 
The `toread.yaml` stores in a plain text format the QIDs to be read. The file can be edited either manually or via automatic queries (not discussed in this report for conciseness).
The `index.yaml` file holds an hierarchy of topics of interest for the researcher, to be used for indexing/tagging the notes in the way required by the topic of research. This file plays the role of  Umberto Eco's "work plan", with the topics of interest for the academic. [@wikidata:Q3684178]
Finally, the `config.yaml` contains options to customize the system for advanced use. For example, the user can configure shortcuts for different reading lists. 
For example, in my `to read.yaml` file there are two reading lists, one entitled `Cell types` header and another entitled `Biocuration`. 
My `config.yaml` contains the following snippet: 
```yaml
lists:
# - shortcut: Title of header in toread.md  
  ct: Cell types
  bioc: Biocuration
```
The `config.yaml` shortcuts are used as arguments by the `bib pop` command, where `$ bib pop ct` retrieves an article from the "Cell types" list, while `$ bib pop bioc` retrieves an article from the "Biocuration" list.  

The Wikidata bib framework is coupled with a discipline of daily reading.
The discipline is inspired by Robert Cecil Martin's description of Test Driven Development in the book "Clean Code", which includes not only a technical description but a _school of thought_ of how software development might be approached.  [@wikidata:Q109996684]
Every day, I read one article of each list, using the notetaking station displayed in Figure @fig: notetaking.
The constancy of reading allows steady coverage of the relevant literature. 
While the discipline has worked for this research project, it is not required to use the Wikidata Bib system.

The notetaking station of Wikidata Bib, opened in Virtual Studio Code, is depicted on Figure @fig: notetaking A.
The title and publication dates are displayed, and the reading process entails copying snippets from the text to the "Highlights" session. 
Copying the highlights into plain text makes the sections of interest searchable via command line using `grep` (https://en.wikipedia.org/w/wiki/Grep).
Comments can be added either in the comment section or inline, alongside the highlights. 
Even though there is no enforcement, our recommendation is to use HTML comment syntax `<!-- Comment goes here -->` to differentiate from highlights.
Visual Studio offers a default shortcut (CTRL + /) for adding comments, making the process seamless.

The Wikidata Bib framework also includes, whenever possible, an improvement of the metadata about the article on Wikidata. 
In @fig: notetaking B are shown the links included in the dashboard. 
A link to a Scholia [@wikidata:Q41799194] profile allows identification of related articles from a series of pre-made SPARQL queries probing bibliography data on Wikidata.
While Scholia provides an overview of a given article, it does not allow direct curation of the metadata.
For that, two links are provided, one to Wikidata and one to Author Disambiguator [@url:https://www.wikidata.org/wiki/Wikidata:Tools/Author_Disambiguator].
By accessing the Wikidata page for the entity, one can add new triples, for example, curating authors and topics of the article, which are then used by Scholia and by Wikidata Bib's dashboard. 
Author Disambiguator is a wrapper of an Wikimedia API that facilitates disambiguating author names to unique identifiers on Wikidata, thus feeding the public knowledge graph of publication and authors.  
Finally, a link to the article's DOI or full-text URL is provided and serves as a fallback when the automatic download fails. 
Of note, while the metadata curation has a technical benefit to Wikidata and the dashboard, it also plays a theoretical role. 
By curating metadata on authors, the user of Wikidata Bib can better understand the people they read, and expand their metascientific perspective on their domain of interest. 
Furthermore, by embedding the metascientific curation in the reading process WikidataBib directs the user to get acquainted with the people producing the science they read, thus contributing to a more complete perspective of the research process.

![Wikidata Bib's platform for note taking](images/note_taking_station_annotated_with_links.png){#fig:notetaking}

The source code for Wikidata Bib is available at <https://github.com/lubianat/wikidata_bib>. 


## Wikidata Bib as a dashboard 

The Wikidata Bib system also enables the reader to get statistics on their readings. 
Two simple databases are stored on the GitHub repository:
 * `read.ttl` - An RDF document recording the dates in which each article was read. 
 * `read.csv` - An simple, human-readable index connecting QIDs with article titles. 
The CSV file is only stored for accountability and as a quick way to glance at the titles read. 
The .ttl file, on the other hand, is processed by the `update_dashboard.py` script to render 4 different HTML files under the `docs/` folder: 
 - `index.html`
 - `last_day.html`
 - `past_week.html`
 - `past_month.html`
All files are displayed in a GitHub pages static website. 
In the case of this work, they are displayed at <https://lubianat.github.io/wikidata_bib/>. 

To organize the code for rendering the dashboard, we created a python package, wbib, and deposited it in PyPi, making it available via `pip`. [@url:https://pypi.org/project/wbib].
The package implements the logic for rendering complex Wikidata-based academic dashboards and is available in GitHub at <https://github.com/lubianat/wbib>. 
It allows the user to build dashboards based on Wikidata records of information such as gender of authors, the region of author's institutions, topics of articles and similar metascientific information. 
The dashboard is composed of SPARQL queries written for the Wikidata Query Service [@url:https://query.wikidata.org] 
It also allows users to feed an arbitrary list of articles and obtain a custom dashboard. 
Wikidata Bib obtains the HTML dashboards after feeding wbib the lists of articles read in total (`index.html`) or in pre-determined time spans (`last_day.html`, `past_week.html` and `past_month.html` )

![Wikidata Bib queries for institutions of authors and most-read venues](images/wikidata_bib_display.png){#fig:dashboard width="85%"}

The dashboard includes not only a basic list of read articles, but also statistics on most read authors and most-read venues. 
It also displays an interactive map of the institutions of articles read, permitting a glance at geographic biases in activities. 
An example of queries is shown in @fig:dashboard.
As the queries are rendered live, they evolve in quality with the growth of Wikidata. 
Finally, the clean 5-star-open data format enables users to adapt the queries to include different aspects of Wikidata. 
For example, table @tbl:articles_read_hca showcases 10 articles that (1) I have read in the past year and (2) were authored by a speaker of the 1st Human Cell Atlas Latin America Single Cell RNA-seqData Analysis Workshop [@url:https://www.humancellatlas.org/hca-latin-america-2021-workshop]. 
One practical application that the dashboard enables, thus, is to identify people in an event, institution or location that the user has read before, therefore catalyzing the possibility of collaborations. 
Anecdotally, this strategy was tested successfully at Biohackathon Europe 2021 [@url:https://biohackathon-europe.org], where we used the system both to identify possible collaborators and as a conversation starter. 

|workLabel                                                                                        |authors                                                                   |
|-------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------|
|A promoter-level mammalian expression atlas                                                      |Jay W Shin                                                                |
|Single-cell RNA-seq reveals new types of human blood dendritic cells, monocytes, and progenitors.|Muzlifah Haniffa                                                          |
|The Human Cell Atlas.                                                                            |Musa Mhlanga, Jay W Shin, Muzlifah Haniffa, Menna R Clatworthy, Dana Pe'er|
|The Human Cell Atlas: Technical approaches and challenges.                                       |Jay W Shin                                                                |
|Innate Immune Landscape in Early Lung Adenocarcinoma by Paired Single-Cell Analyses.             |Dana Pe'er |
|Single cell RNA sequencing of human liver reveals distinct intrahepatic macrophage populations   |Sonya A MacParland                                                        |
|Single-cell reconstruction of the early maternal-fetal interface in humans                       |Muzlifah Haniffa                                                          |
|Distinct microbial and immune niches of the human colon                                          |Rasa Elmentaite, Menna R Clatworthy                                       |
|A cell atlas of human thymic development defines T cell repertoire formation                     |Muzlifah Haniffa, Menna R Clatworthy                                      |
|Decoding human fetal liver haematopoiesis                                                        |Muzlifah Haniffa                                                          |

Table: Articles read by Tiago Lubiana before 8 December 2021 in which an author was a speaker at HCA Latin America
{#tbl:articles_read_hca}                                                        |

## Wikidata Bib for curation of cells to Wikidata

The Wikidata Bib system was devised originally to allow an overview of the fields of cell classification and biocuration. 
However, during the process, it was also repurposed for biocuration of new cell classes in Wikidata.
By fast-tracking the reading of new articles, Wikidata Bib enables an efficient parsing of the literature and, thus, the identification of previously uncatalogued cell types. 

Articles read with Wikidata Bib were screened to mention cell types absent from Wikidata. 
As discussed in the chapter about the concept of cell type, we considered a "cell type" as any class of cells described by a domain expert with evidence of the reality of its instances.
When a mention of such a class appears in an article, I first verify Wikidata for the existence of a related class. 
If it is absent from the platform, I enter a class name, alongside a superclass, and a QID in a Google Spreadsheet, as shown in Figure @fig:biocuration_of_cells.

The information from the spreadsheet is pulled by a python script and processed locally with a series of dictionaries that match common terms to Wikidata IDs. 
In the example shown in Figure @fig:biocuration_of_cells, the string "endothelial cell" was matched against a manually curated dictionary to the Wikidata entry [Q11394395](https://www.wikidata.org/wiki/Q11394395), the representation of that concept on Wikidata. 
After reconciling the data, the script uses the Wikidata Integrator python package [@url:https://github.com/SuLab/WikidataIntegrator] to insert the new entries on the Wikidata database. 
The code for integrating a Google Spreadsheet to Wikidata is available at <https://github.com/lubianat/wikidata_cell_curation>. 

![Wikidata Bib was coupled with a biocuration framework for cell types](images/biocuration_of_cells.png){#fig:biocuration_of_cells width="85%"}

<!-- UPDATE THE STATISTICS 
Jupyter Notebook: https://colab.research.google.com/drive/1GvQXOs51_U8icdGMtKXMeLOXKM8pXWet
-->

Wikidata contains 4075 subclasses of "cell ([Q7868](https://www.wikidata.org/wiki/Q7868))" as of 16 of June of 2022. 
From those, 550 cell classes are specific for humans, and 318 are specific for mice.  
Currently Wikidata has more cell classes than the Cell Ontology, which lists 2577 classes. 
It is worth noticing that classes on the Cell Ontology are added after careful consideration by ontologists and domain experts and should be considered of higher quality than the ones on Wikidata. 

From the 4075 cell classes on Wikidata, 3946 (96.8%) have been edited somehow by User:TiagoLubiana, and 2737 (67.2%) have been created by User:TiagoLubiana. 
Edits included adding multilanguage labels, connecting a dangling Wikipedia page to the cell subclass hierarchy, adding identifiers, images, markers, and other pieces of information. 
Approximatedly 430 hundred terms were added via manual curation based on PanglaoDB entries, while the remaining 2307 entries were created either via Wikidata's web interface or via the curation workflow described in this chapter. 
These statistics are demonstration of how the curation system efficiently contributes to the status of cell type information on Wikidata.

<!-- UPDATE THE STATISTICS -->

As mentioned by Aviv Regev in the Human Cell Atlas General Meeting 2021, "it's everyone's collective
responsibility to participate in the annotation efforts, because that relies on domain expertise. To really tease apart things and give
them names. Until we have names, people will have really a hard time working with things in biology. "[@url:"https://youtu.be/orbqAqV9mQM?list=PLkef4SGmngdZanrQtu_g9kiuAcYczzDfn&t=1959"]"
We hope that by developing simplified curation tools we will engage more domain experts into the curation efforts. 

## Guidelines for reporting new cell types  {.page_break_before}

# Additional Work {.page_break_before}

## Collaborations and manuscripts

### fcoex 

During the initial course of this PhD work, we also completed the development and reporting of _fcoex_, an R package for investigating cellular phenotypes using co-expression networks. [@url:https://www.bioconductor.org/packages/release/bioc/html/fcoex.html] The software was maintained to withstand new releases of dependencies and new R version and was published as a preprint on biorxiv. [@doi:10.1101/2021.12.07.471603v1]
 
### Wikidata Bots

Alongside the editing of cell-type information on Wikidata, I have joined different efforts to improve biological information on Wikidata.
I have collaborated with the ComplexPortal curators as part of the Virtual Elixir BioHackathon 2020 (https://github.com/virtual-biohackathons/covid-19-bh20/wiki) and for the following year, to build a Wikidata Bot to integrate information on protein complexes to Wikidata. An overview of the Wikidata integration is in Figure @fig:complexportal, presented in an article published in Nucleic Acid Research (re-use of the image and legend possible under the CC-BY license of the article). [@wikidata:Q109348309]

![Complex Portal and Wikidata. (A) example of an entry assertion in Wikidata with provenance pointing to Complex Portal (Q104836061). (B) The number of protein complexes in Wikidata per taxon (<https://w.wiki/3ggX>). (C) Subset of Wikidata connected to the SARS-CoV-2 polymerase complex (<https://w.wiki/3eta>).](https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/nar/PAP/10.1093_nar_gkab991/2/gkab991fig3.jpeg?Expires=1641821957&Signature=RK-es18S~Qh6vGQE~61i6u4prMuij8kVTbrjN6WUJLfYHOAhUhx9qQorBxROohjLLxbHvZ2YK9e7EwlI9HjVeNoGZ2PJs0Pv78Y31MdZLY8FeLYI2E4azwrqRyv9q0AH8QL3RorWZV1AhOb9bl-44Mr97Q~9MWzeTDnQQbxpCnGLG~YoG49kocD5KE~dmTSQdkXBU7kZnuGM1NPqMHo5ZDUoCRFwmTbLvd4kXoH~6CTyqx4ruQRIO-ks4Q0FUjYnxGX18gCFvoO2LMPZUPrQjTgD35RrVAkLs91Ur6BQHUuZ-ZsE3Iag3mBDu27vT9Jr93bOj0i9wz~surTtEGATgw__&Key-Pair-Id=APKAIE5G5CRDK6RD3PGA){#fig:complexportal width="85%}

I have also collaborated with the Cellosaurus database [@wikidata:Q54370168] to revive the CellosaurusBot [@url:https://www.wikidata.org/wiki/User:CellosaurusBot], responsible for updating the metadata on more than 100,000 cell lines on Wikidata. The bot code, written in Python, was refactored entirely and runs semi-automatically after the Cellosaurus database was released. A write-up of the integration is in progress and is planned for release/submission in the second semester of 2022. 

### WiseCube - enterprise biomedical question and answering

During a part of this project, I have worked part-time as a consultant for the Wisecube company, based in Seattle, United States. [@url:https://www.wisecube.ai]
The job was approved by FAPESP and consisted mainly in writing SPARQL queries that probe Wikidata for answers to the questions posed by the BioASQ competition. [@wikidata:Q28646342]
It also entailed on-demand curation of biomedical topics on Wikidata based on requests by pharmaceutical companies as well as the development of dashboards targeted at providing insights to customers. 


## Awards and participation in events

* (Nov - 2021) Managed a project during BioHackathon Europe 2021, in Barcelona, Spain, on the representation of ELIXIR information on Wikidata. [@url:https://github.com/elixir-europe/biohackathon-projects-2021/tree/main/projects/32]
* (May - 2022) Presented the WikidataBib in a talk the 1st UK Local Biocuration conference, which got awarded as the Runner-Up Best Talk 

